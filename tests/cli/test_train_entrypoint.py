# Copyright (c) MIST Imaging LLC.
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Tests for MIST training entrypoint CLI."""
from typing import List, Tuple
import argparse
import json
import os
from pathlib import Path
import pandas as pd
import pytest

# MIST imports.
import mist.cli.train_entrypoint as entry


# =============================================================================
# Helpers and minimal patching for ArgParser.
# =============================================================================

# pylint: disable=protected-access
class _DummyTrainer:
    """A tiny stub trainer that records if fit() was called."""

    def __init__(self, ns: argparse.Namespace) -> None:
        """Initialize and capture the namespace."""
        self.ns = ns
        self.fit_called = False

    def fit(self) -> None:
        """Mark that fit() was called."""
        self.fit_called = True


def _patch_minimal_cli(monkeypatch) -> None:
    """Patch argmod.* to provide a minimal, deterministic CLI."""

    def _mk_parser(**kwargs):
        return argparse.ArgumentParser(**kwargs)

    def _add_train_args(parser: argparse.ArgumentParser) -> None:
        parser.add_argument("--results", type=str, default=None)
        parser.add_argument("--numpy", type=str, default=None)
        parser.add_argument("--gpus", nargs="+", type=int, default=[-1])
        parser.add_argument("--overwrite", action="store_true")

    monkeypatch.setattr(entry.argmod, "ArgParser", _mk_parser, raising=True)
    monkeypatch.setattr(
        entry.argmod, "add_train_args", _add_train_args, raising=True
    )


class _NoGrad:
    """Minimal no_grad context manager."""

    def __enter__(self):
        """Enter context."""
        return self

    def __exit__(self, exc_type, exc, tb):
        """Exit context."""
        return False


# =============================================================================
# Tests for _parse_train_args.
# =============================================================================


def test_parse_train_args_fallbacks(tmp_path, monkeypatch):
    """It sets default ./results and ./numpy when not provided."""
    _patch_minimal_cli(monkeypatch)
    monkeypatch.chdir(tmp_path)

    ns = entry._parse_train_args(argv=[])
    assert Path(ns.results) == (tmp_path / "results").resolve()
    assert Path(ns.numpy) == (tmp_path / "numpy").resolve()


def test_parse_train_args_explicit(tmp_path, monkeypatch):
    """It keeps explicit --results and --numpy values."""
    _patch_minimal_cli(monkeypatch)
    res = tmp_path / "out"
    npy = tmp_path / "np"
    ns = entry._parse_train_args(
        argv=["--results", str(res), "--numpy", str(npy)]
    )
    assert ns.results == str(res)
    assert ns.numpy == str(npy)


# =============================================================================
# Tests for _ensure_required_artifacts.
# =============================================================================


def _write_required_files(base: Path, include_test: bool = False) -> None:
    """Write required results files; optionally include test_paths.csv."""
    (base / "config.json").write_text(json.dumps({"k": "v"}))
    (base / "train_paths.csv").write_text("id,mask,ct\n0,m0,i0\n")
    (base / "fg_bboxes.csv").write_text("id,x_start,x_end\n0,1,2\n")
    if include_test:
        (base / "test_paths.csv").write_text("id,mask,ct\n9,mt,it\n")


def _ensure_numpy_dirs(numpy_dir: Path) -> None:
    """Create numpy/images and numpy/labels subdirs."""
    (numpy_dir / "images").mkdir(parents=True, exist_ok=True)
    (numpy_dir / "labels").mkdir(parents=True, exist_ok=True)


def test_ensure_required_artifacts_happy_and_test_flag(tmp_path):
    """It returns (results_dir, has_test_paths) when structure is valid."""
    results_dir = tmp_path / "results"
    numpy_dir = tmp_path / "numpy"
    results_dir.mkdir()
    numpy_dir.mkdir()
    _write_required_files(results_dir, include_test=True)
    _ensure_numpy_dirs(numpy_dir)

    ns = argparse.Namespace(results=str(results_dir), numpy=str(numpy_dir))
    got_results, has_test = entry._ensure_required_artifacts(
        ns
    )
    assert got_results == results_dir.resolve()
    assert has_test is True


def test_ensure_required_artifacts_missing_results_dir(tmp_path):
    """It raises when results directory does not exist."""
    numpy_dir = tmp_path / "numpy"
    numpy_dir.mkdir()
    ns = argparse.Namespace(
        results=str(tmp_path / "missing"), numpy=str(numpy_dir)
    )
    with pytest.raises(FileNotFoundError):
        entry._ensure_required_artifacts(ns)


def test_ensure_required_artifacts_missing_results_files(tmp_path):
    """It raises when required files in results are missing."""
    results_dir = tmp_path / "results"
    numpy_dir = tmp_path / "numpy"
    results_dir.mkdir()
    numpy_dir.mkdir()
    # Write only one file; others missing.
    (results_dir / "config.json").write_text("{}")
    _ensure_numpy_dirs(numpy_dir)

    ns = argparse.Namespace(results=str(results_dir), numpy=str(numpy_dir))
    with pytest.raises(FileNotFoundError) as e:
        entry._ensure_required_artifacts(ns)
    assert "train_paths.csv" in str(e.value) and "fg_bboxes.csv" in str(
        e.value
    )


def test_ensure_required_artifacts_missing_numpy_dirs(tmp_path):
    """It raises when numpy subfolders images/labels are missing."""
    results_dir = tmp_path / "results"
    numpy_dir = tmp_path / "numpy"
    results_dir.mkdir()
    numpy_dir.mkdir()
    _write_required_files(results_dir, include_test=False)
    # Intentionally do not create images/labels under numpy

    ns = argparse.Namespace(results=str(results_dir), numpy=str(numpy_dir))
    with pytest.raises(FileNotFoundError) as e:
        entry._ensure_required_artifacts(ns)
    assert "images" in str(e.value) and "labels" in str(e.value)


def test_ensure_required_artifacts_missing_numpy_dir(tmp_path):
    """It raises when the NumPy directory itself does not exist."""
    results_dir = tmp_path / "results"
    results_dir.mkdir()
    _write_required_files(
        results_dir, include_test=False
    )  # helper from this test module

    ns = argparse.Namespace(
        results=str(results_dir), numpy=str(tmp_path / "numpy_missing")
    )
    with pytest.raises(FileNotFoundError) as e:
        entry._ensure_required_artifacts(ns)
    assert "NumPy directory does not exist" in str(e.value)


def test_ensure_required_artifacts_numpy_path_is_file(tmp_path):
    """It raises when the NumPy path exists but is a file, not a directory."""
    results_dir = tmp_path / "results"
    numpy_path = tmp_path / "numpy_file"
    results_dir.mkdir()
    numpy_path.write_text("not a dir")
    _write_required_files(results_dir, include_test=False)

    ns = argparse.Namespace(results=str(results_dir), numpy=str(numpy_path))
    with pytest.raises(FileNotFoundError) as e:
        entry._ensure_required_artifacts(ns)
    assert "NumPy directory does not exist" in str(e.value)


# =============================================================================
# Tests for _create_train_dirs.
# =============================================================================


@pytest.mark.parametrize("has_test_paths", [False, True])
def test_create_train_dirs_makes_structure(tmp_path, has_test_paths):
    """It creates logs, models, and prediction directories."""
    entry._create_train_dirs(tmp_path, has_test_paths)
    assert (tmp_path / "logs").is_dir()
    assert (tmp_path / "models").is_dir()
    assert (tmp_path / "predictions" / "train" / "raw").is_dir()
    if has_test_paths:
        assert (tmp_path / "predictions" / "test").is_dir()
    else:
        assert not (tmp_path / "predictions" / "test").exists()


# =============================================================================
# Tests for _set_visible_devices.
# =============================================================================


def test_set_visible_devices_no_gpus_raises(monkeypatch):
    """It raises a RuntimeError when torch reports 0 GPUs."""
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 0, raising=True
    )
    with pytest.raises(RuntimeError):
        entry._set_visible_devices(
            argparse.Namespace(gpus=[-1])
        )


@pytest.mark.parametrize("cli_gpus", [None, [], [-1]])
def test_set_visible_devices_all_gpus(monkeypatch, cli_gpus):
    """It sets CUDA_VISIBLE_DEVICES to all indices for None/[]/[-1]."""
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 3, raising=True
    )
    ns = argparse.Namespace(gpus=cli_gpus)
    entry._set_visible_devices(ns)
    assert os.environ.get("CUDA_VISIBLE_DEVICES") == "0,1,2"


def test_set_visible_devices_specific_indices(monkeypatch):
    """It sets CUDA_VISIBLE_DEVICES to the requested indices."""
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 4, raising=True
    )
    ns = argparse.Namespace(gpus=[1, 3])
    entry._set_visible_devices(ns)
    assert os.environ.get("CUDA_VISIBLE_DEVICES") == "1,3"


def test_set_visible_devices_invalid_indices(monkeypatch):
    """It raises ValueError when indices are out of range."""
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 2, raising=True
    )
    ns = argparse.Namespace(gpus=[3])
    with pytest.raises(ValueError):
        entry._set_visible_devices(ns)


# =============================================================================
# Tests for train_entry — integration behavior.
# =============================================================================


def test_train_entry_blocks_existing_results_csv_without_overwrite(
    tmp_path, monkeypatch
):
    """It raises FileExistsError when results.csv exists without --overwrite."""
    _patch_minimal_cli(monkeypatch)

    # Build valid results and numpy trees.
    results_dir = tmp_path / "results"
    numpy_dir = tmp_path / "numpy"
    results_dir.mkdir()
    numpy_dir.mkdir()
    _write_required_files(results_dir, include_test=False)
    _ensure_numpy_dirs(numpy_dir)
    (results_dir / "results.csv").write_text("already here")

    # Ensure no GPU-driven crash by mocking device_count.
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 1, raising=True
    )

    # If trainer is constructed, fail—we should block earlier.
    monkeypatch.setattr(
        entry,
        "Patch3DTrainer",
        lambda _ns: (_ for _ in ()).throw(
            AssertionError("Trainer should not be created")
        ),
        raising=True,
    )

    argv = ["--results", str(results_dir), "--numpy", str(numpy_dir)]
    with pytest.raises(FileExistsError):
        entry.train_entry(argv)


def test_train_entry_happy_path_no_test_empty_eval(tmp_path, monkeypatch):
    """It trains, folds infer, and prints warnings with empty eval pairs."""
    _patch_minimal_cli(monkeypatch)

    # Folder structure & required files (no test_paths.csv).
    results_dir = tmp_path / "results"
    numpy_dir = tmp_path / "numpy"
    results_dir.mkdir()
    numpy_dir.mkdir()
    _write_required_files(results_dir, include_test=False)
    _ensure_numpy_dirs(numpy_dir)

    # Config returned by io.read_json_file.
    config = {
        "training": {"folds": [0, 1]},
        "evaluation": {
            "final_classes": {"background": [0], "foreground": [1]},
            "metrics": ["dice"],
            "params": {"surf_dice_tol": 1.0},
        },
    }
    monkeypatch.setattr(
        entry.io, "read_json_file", lambda p: config, raising=True
    )

    # GPU availability
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 1, raising=True
    )
    monkeypatch.setattr(
        entry.torch, "no_grad", lambda: _NoGrad(), raising=True
    )

    # Trainer stub.
    created = {}
    monkeypatch.setattr(
        entry,
        "Patch3DTrainer",
        lambda ns: created.setdefault("trainer", _DummyTrainer(ns)),
        raising=True,
    )

    # Record folds for test_on_fold calls.
    folds_called: List[int] = []
    monkeypatch.setattr(
        entry,
        "test_on_fold",
        lambda ns, f: folds_called.append(f),
        raising=True,
    )

    # Evaluation dataframe is empty, with warnings.
    monkeypatch.setattr(
        entry.evaluation_utils,
        "build_evaluation_dataframe",
        lambda **kwargs: (pd.DataFrame(), "[warn] something"),
        raising=True,
    )

    # Capture console prints.
    logs = []
    monkeypatch.setattr(
        "rich.console.Console.print",
        lambda self, *a, **k: logs.append(" ".join(map(str, a))),
    )

    # Evaluator should not be called since dataframe is empty.
    monkeypatch.setattr(
        entry,
        "Evaluator",
        lambda *a, **k: (_ for _ in ()).throw(
            AssertionError("Evaluator should not be constructed")
        ),
        raising=True,
    )

    argv = [
        "--results", str(results_dir),
        "--numpy", str(numpy_dir),
        "--overwrite",
    ]
    entry.train_entry(argv)

    # Trainer.fit called.
    assert created["trainer"].fit_called is True
    # Folds inferred.
    assert folds_called == [0, 1]
    # Warnings surfaced, and empty-eval message printed.
    assert any("warn" in m.lower() for m in logs)
    assert any("No valid prediction-mask pairs" in m for m in logs)
    # Predictions/train/raw created.
    assert (results_dir / "predictions" / "train" / "raw").is_dir()
    # No evaluation_paths.csv written.
    assert not (results_dir / "evaluation_paths.csv").exists()


def test_train_entry_happy_path_with_eval_and_test_infer(
    tmp_path, monkeypatch
):
    """Run eval and test inference when pairs exist and test set present."""
    _patch_minimal_cli(monkeypatch)

    # Files and dirs including test paths.
    results_dir = tmp_path / "results"
    numpy_dir = tmp_path / "numpy"
    results_dir.mkdir()
    numpy_dir.mkdir()
    _write_required_files(results_dir, include_test=True)
    _ensure_numpy_dirs(numpy_dir)

    # Config for folds/eval params.
    config = {
        "training": {"folds": [2]},
        "evaluation": {
            "final_classes": {"background": [0], "foreground": [1]},
            "metrics": ["dice"],
            "params": {"surf_dice_tol": 0.5},
        },
    }
    (results_dir / "config.json").write_text(json.dumps(config))
    monkeypatch.setattr(
        entry.io, "read_json_file", lambda p: config, raising=True
    )

    # GPU availability.
    monkeypatch.setattr(
        entry.torch.cuda, "device_count", lambda: 2, raising=True
    )
    monkeypatch.setattr(
        entry.torch, "no_grad", lambda: _NoGrad(), raising=True
    )

    # Trainer stub.
    trainer = _DummyTrainer(argparse.Namespace())
    monkeypatch.setattr(
        entry, "Patch3DTrainer", lambda ns: trainer, raising=True
    )

    # Fold test tracker.
    folds_called: List[int] = []
    monkeypatch.setattr(
        entry,
        "test_on_fold",
        lambda ns, f: folds_called.append(f),
        raising=True,
    )

    # Non-empty evaluation dataframe.
    eval_df = pd.DataFrame(
        {"prediction": ["p.nii.gz"], "mask": ["m.nii.gz"], "id": [0]}
    )
    monkeypatch.setattr(
        entry.evaluation_utils,
        "build_evaluation_dataframe",
        lambda **kwargs: (eval_df, ""),
        raising=True,
    )

    # Evaluator stub that writes results.csv.
    runs = {"called": False}

    def _mk_eval(
        filepaths_dataframe,
        evaluation_classes,
        output_csv_path,
        selected_metrics,
        surf_dice_tol,
    ):
        class _E:
            def run(self_nonlocal):
                runs["called"] = True
                Path(output_csv_path).write_text("metric,value\n")

        return _E()

    monkeypatch.setattr(entry, "Evaluator", _mk_eval, raising=True)

    # test inference: patch read_csv + infer runner.
    test_df = pd.DataFrame({"id": [9]})
    monkeypatch.setattr(pd, "read_csv", lambda p: test_df, raising=True)

    infer_calls: List[Tuple[str, str]] = []

    def _infer_from_dataframe(
        paths_dataframe,
        output_directory,
        mist_configuration,
        models_directory,
        postprocessing_strategy_filepath,
        device,
    ):
        infer_calls.append((output_directory, models_directory))
        p = Path(output_directory)
        # output_directory is .../predictions/test (the folder itself).
        assert p.name == "test"
        assert p.parent.name == "predictions"
        assert Path(models_directory).name == "models"

    monkeypatch.setattr(
        entry, "infer_from_dataframe", _infer_from_dataframe, raising=True
    )

    argv = [
        "--results", str(results_dir),
        "--numpy", str(numpy_dir),
        "--overwrite",
    ]
    entry.train_entry(argv)

    # Trainer.fit called.
    assert trainer.fit_called is True
    # Fold tested
    assert folds_called == [2]
    # Evaluation ran and produced results.csv.
    assert runs["called"] is True
    assert (results_dir / "results.csv").is_file()
    # evaluation_paths.csv written.
    assert (results_dir / "evaluation_paths.csv").is_file()
    # Test inference invoked with expected directories.
    assert len(infer_calls) == 1

Advanced Topics
===

## Configuration file

MIST uses a centralized **`config.json`** file to store dataset information,
preprocessing parameters, model settings, training hyperparameters, inference
strategies, and evaluation metrics.  

This file is automatically generated during the **analysis step** and is
**required** for all subsequent stages of the pipeline (preprocessing, training,
inference, and evaluation).  

If you want to **customize training or inference**, you can edit this file
directly. For example, you can safely change the network architecture,
loss function, or optimizer.

Editing preprocessing is possible but usually not recommended unless you know
your dataset requires different resampling or normalization. If, for example,
you want to apply a different normalization scheme or preprocessing techniques
that are not available in MIST, then we recommend applying your technique to
your images and saving them as NIfTI files. Then, you can either set the
`preprocessing.skip` entry to `true` or use the `--no-preprocess` flag with the
`mist_run_all` or `mist_preprocess` commands.

Below is an example of a valid `config.json` file.

```json
{
  "mist_version": "1.0.0b0",

  "dataset_info": {
    "task": "ivygap",
    "modality": "mr",
    "images": ["t1", "t2", "tc", "fl"],
    "labels": [0, 1, 2, 4]
  },

  "preprocessing": {
    "skip": false,
    "target_spacing": [1.0, 1.0, 1.0],
    "crop_to_foreground": true,
    "median_resampled_image_size": [135, 174, 139],
    "normalize_with_nonzero_mask": true,
    "ct_normalization": {
      "window_min": null,
      "window_max": null,
      "z_score_mean": null,
      "z_score_std": null
    },
    "compute_dtms": false,
    "normalize_dtms": true
  },

  "model": {
    "architecture": "nnunet",
    "params": {
      "in_channels": 4,
      "out_channels": 4,
      "patch_size": [128, 128, 128],
      "target_spacing": [1.0, 1.0, 1.0],
      "use_deep_supervision": true,
      "use_residual_blocks": true,
      "use_pocket_model": true
    }
  },

  "training": {
    "seed": 42,
    "nfolds": 5,
    "folds": [0, 1, 2, 3, 4],
    "val_percent": 0.0,
    "epochs": 1000,
    "min_steps_per_epoch": 250,
    "batch_size_per_gpu": 2,
    "dali_foreground_prob": 0.6,

    "loss": {
      "name": "dice_ce",
      "params": {
        "use_dtms": false,
        "composite_loss_weighting": null
      }
    },

    "optimizer": "adam",
    "learning_rate": 0.001,
    "lr_scheduler": "cosine",
    "l2_penalty": 1e-05,
    "amp": true,

    "augmentation": {
      "enabled": true,
      "transforms": {
        "flips": true,
        "zoom": true,
        "noise": true,
        "blur": true,
        "brightness": true,
        "contrast": true
      }
    },

    "hardware": {
      "num_gpus": 1,
      "num_cpu_workers": 8,
      "master_addr": "localhost",
      "master_port": 12345,
      "communication_backend": "nccl"
    }
  },

  "inference": {
    "inferer": {
      "name": "sliding_window",
      "params": {
        "patch_size": [128, 128, 128],
        "patch_blend_mode": "gaussian",
        "patch_overlap": 0.5
      }
    },
    "ensemble": {
      "strategy": "mean"
    },
    "tta": {
      "enabled": true,
      "strategy": "all_flips"
    }
  },

  "evaluation": {
    "metrics": ["dice", "haus95"],
    "final_classes": {
      "wt": [1, 2, 4],
      "tc": [1, 4],
      "et": [4]
    },
    "params": {
      "surf_dice_tol": 1.0
    }
  }
}
```

The `config.json` file is divided into several major sections. Each section controls 
a different part of the MIST pipeline:

| Section          | Purpose                                                                                              |
|------------------|------------------------------------------------------------------------------------------------------|
| `mist_version`   | Tracks the version of MIST used to generate the configuration.                                       |
| `dataset_info`   | Metadata about the dataset, including task name, modality, image inputs, and label values.           |
| `preprocessing`  | Defines how raw images are resampled, cropped, and normalized before training.                       |
| `model`          | Specifies the model architecture and its hyperparameters.                                            |
| `training`       | Controls training loop, cross-validation folds, loss function, optimizer, and augmentations.         |
| `inference`      | Settings for inference, including sliding-window parameters, ensembling, and test-time augmentation. |
| `evaluation`     | Metrics and class definitions for model evaluation.                                                  |

## Network architectures

MIST's default network architecture is the 3D nnUNet with a residual encoder and
two deep supervision heads. However the following architectures are also
available:

| Architecture      | `model.architecture`    |
|-------------------|-------------------------|
| nnUNet (default)  | `nnunet`                |
| MedNeXt (small)   | `mednext-small`         |
| MedNeXt (base)    | `mednext-base`          |
| MedNeXt (medium)  | `mednext-medium`        |
| MedNeXt (large)   | `mednext-large`         |
| FMG-Net           | `fmgnet`                |
| W-Net             | `wnet`                  |

The architecture can be specified with the `--model` flag in the `mist_run_all`
or `mist_train` commands or directly edited in the `config.json` file. 

Pocket versions of the nnUNet and MedNeXt models are available. To make this
modification to these architectures, use the `--pocket` flag with the
`mist_run_all` or `mist_train` commands or set the
`model.params.use_pocket_model` attribute to `true`.

### Example

Run the MIST training pipeline with a pocket MedNeXt base model.

```console
mist_train --numpy /path/to/preprocessed/npy/files \
           --results /path/to/results/folder \
           --model mednext-base \
           --pocket
```

## Loss functions

MIST's default loss function is the Dice with cross entropy loss function.
However, the following loss functions are available in MIST:

| Loss function          | `training.loss.name`  |
|------------------------|-----------------------|
| Dice w/ cross entropy  | `dice_ce`             |
| Dice                   | `dice`                |
| Boundary               | `bl`                  |
| One-sided Hausdorff    | `hdos`                |
| Generalized surface    | `gsl`                 |
| clDice                 | `cldice`              |

The loss function can be specified with the `--loss` flag in the `mist_run_all`
or `mist_train` commands or set in the `config.json` file under the
`training.loss.name` attribute.

### Using distance transform maps

The boundary, one-sided Hausdorff, and generalized surface loss functions use
distance transform maps (DTMs). To use these losses, enable DTM computation for
ground-truth labels with the `--compute-dtms` flag in the `mist_run_all` or
`mist_preprocess` commands. Additionally, enable the use of DTMs during training
with the `--use-dtms` flag in `mist_run_all` or `mist_train`. You can also set
`preprocessing.compute_dtms` and `training.loss.params.use_dtms` to `true` in
the `config.json` file.

By default, MIST normalizes DTMs per label to the interval `[-1, 1]`, which
helps stabilize training (especially with AMP). To disable this, set
`preprocessing.normalize_dtms` to `false` in `config.json`.

### Composite loss weighting schedules

Boundary-based losses are often combined with region-based losses (e.g., Dice)
via a weighted sum:

$$
\alpha \mathcal{L}_{\text{region}} + (1 - \alpha)\mathcal{L}_{\text{boundary}}.
$$

MIST provides two schedules for $\alpha$: `constant` and `linear`.

- `constant`: fixes $\alpha$ = `0.5`.
- `linear`: uses $\alpha$ = `1.0` for the first five epochs, then decreases
  linearly to `0.0` by the final epoch.

Set this with `--composite-loss-weighting` or
`training.loss.params.composite_loss_weighting` in `config.json`.

The table below summarizes the recommended use cases for composite loss
weighting for different loss functions:

| Loss     | Schedule   | Use DTMs? | Normalize DTMs? |
|----------|------------|-----------|-----------------|
| `bl`     | `linear`   | Yes       | Yes             |
| `hdos`   | `linear`   | Yes       | Yes             |
| `gsl`    | `linear`   | Yes       | Yes             |
| `cldice` | `constant` | No        | No              |

### Example

Run the entire MIST pipeline with the generalized surface loss and use a linear
composite loss weighting schedule:

```console
mist_run_all --data /path/to/dataset.json \
             --numpy /path/to/preprocessed/npy/files \
             --results /path/to/results/folder \
             --loss gsl \
             --compute-dtms \
             --use-dtms \
             --composite-loss-weighting linear
```

Run the MIST training pipeline with the boundary loss and use a constant
weighting schedule. This assumes that the DTMs have already been computed using
the `mist_preprocess` command.

```console
mist_train --numpy /path/to/preprocessed/npy/files \
           --results /path/to/results/folder \
           --loss bl \
           --use-dtms \
           --composite-loss-weighting constant
```

Run the MIST training pipeline with the clDice loss and use a constant weighting
schedule. The clDice does **not** use DTMs, but does use composite loss
weighting.

```console
mist_train --numpy /path/to/preprocessed/npy/files \
           --results /path/to/results/folder \
           --loss cldice \
           --composite-loss-weighting constant
```

## Optimizers

MIST supports several optimizers commonly used in medical image segmentation:

| Optimizer  | `training.optimizer` |
|------------|----------------------|
| Adam       | `adam`               |
| AdamW      | `adamw`              |
| SGD        | `sgd`                |

The learning rate can be adjusted with `training.learning_rate` entry in the
`config.json` file. Weight decay is set with `training.l2_penalty`. These
parameters can also be set with the `--learning-rate` and `--l2-penalty` flags
in the `mist_run_all` or `mist_train` commands.

### Example

Run the MIST training pipeline with the AdamW and adjust the L2 penalty.

```console
mist_train --numpy /path/to/preprocessed/npy/files \
           --results /path/to/results/folder \
           --optimizer adamw \
           --l2-penalty 0.0001
```

## Learning rate schedulers

Learning rate scheduling can significantly affect convergence. MIST includes the 
following schedulers:

| Scheduler                 | `training.lr_scheduler` |
|---------------------------|-------------------------|
| Cosine Annealing          | `cosine`                |
| Polynomial decay          | `polynomial`            |
| Constant                  | `constant`              |

The scheduler can be specified with the `--lr-scheduler` flag or by editing
`training.lr_scheduler` in the `config.json` file.

The `polynomial` learning rate schedule uses a fixed decay rate of `0.9`. Future
versions of MIST will make this value configurable.

### Example

Run the MIST training pipeline with a polynomial learning schedule with a
higher initial learning rate.

```console
mist_train --numpy /path/to/preprocessed/npy/files \
           --results /path/to/results/folder \
           --learning-rate 0.01 \
           --lr-scheduler polynomial
```

## Docker
The MIST package is also available as a Docker image. Start by pulling the
`mistmedical/mist` image from DockerHub:

```console
docker pull mistmedical/mist:latest
```

Use the following command to run an interactive Docker container with the MIST
package:

```console
docker run --rm -it -u $(id -u):$(id -g) \
           --gpus all \
           --ipc=host \
           --ulimit memlock=-1 \
           --ulimit stack=67108864 \
           -v /your/working/directory/:/workspace \
           mistmedical/mist:latest
```

From there, you can run any of the commands described above inside the Docker
container. Additionally, you can use the Docker entrypoint command to run any
of the MIST scripts.

### Custom cross validation

By default, MIST uses a **random five-fold cross validation** split. The folds
are stored in the file `./results/train_paths.csv`, which maps each case to its
assigned fold.

You can manually edit this file to customize how folds are assigned. For
example, if you want to use **six folds based on data from different
institutions**, you can adjust the `fold` column in `train_paths.csv`
accordingly and then update the `nfolds` entry in the configuration file to `6`.

This flexibility allows you to:

- Control exactly which patients go into which fold.  
- Increase or decrease the number of folds beyond the default five.
- Partition folds according to metadata (e.g., scanner type, acquisition site,
  or institution) rather than random splits.

Another feature is the ability to adjust the train/validation/test split via the
`val_percent` entry in the configuration file, which takes values from `0.0` to
`1.0`. By default it is `0.0`, meaning the entire held-out fold is used for both
validation and test. For example, setting `0.025` holds out 2.5% of the training
set for validation while the held-out fold remains the independent test set.

The `--val-percent` flag can also be used to override this setting at the
command line when running `mist_run_all` or `mist_train`. This makes it easy to
experiment with different validation splits without manually editing the
configuration file.

### Example

Run the MIST training pipeline with a 5% validation split in addition to the
held-out fold:

```console
mist_train --numpy /path/to/preprocessed/npy/files \
           --results /path/to/results/folder \
           --val-percent 0.05
```

## Data Augmentation

Data augmentation is controlled through the `training.augmentation` section of
the `config.json` file. By default, augmentation is enabled with a standard set
of spatial and intensity transforms designed for medical imaging. The available
augmentation transforms are:

| Transform    | Description                                      | Config key   |
|--------------|--------------------------------------------------|--------------|
| Flips        | Random spatial flips along x, y, or z axes       | `flips`      |
| Zoom         | Random zoom in/out with interpolation            | `zoom`       |
| Noise        | Additive Gaussian noise to the image             | `noise`      |
| Blurring     | Random Gaussian blurring                         | `blur`       |
| Brightness   | Random brightness scaling                        | `brightness` |
| Contrast     | Random contrast adjustment                       | `contrast`   |

### How to customize

- To **disable all augmentation**, set `"enabled": false`.  
- To **toggle individual transforms**, set the corresponding key to `true` or
`false`.
- Augmentation is applied only during training. Test-time augmentation is
controlled in the `inference` section of the configuration file.

### Example

Disable noise and blur while keeping other augmentations active:

```json
"augmentation": {
  "enabled": true,
  "transforms": {
    "flips": true,
    "zoom": true,
    "noise": false,
    "blur": false,
    "brightness": true,
    "contrast": true
  }
}
```

## Foreground sampling probability

The `training.dali_foreground_prob` key controls the probability of sampling a
training patch that contains foreground (non-zero labels). This is useful in
medical segmentation tasks where most voxels are background, ensuring the model
sees a balanced mix of positive and negative regions during training.

- A value of `0.0` means **no preference** — patches are sampled uniformly from
  the entire volume.
- A value of `1.0` means **always prefer foreground** — every sampled patch will
  contain at least some non-zero label.
- Intermediate values (e.g., `0.6`) bias sampling toward foreground while still
  including background-only patches.

**Default:** `0.6`

Adjusting this value can improve convergence on highly imbalanced datasets,
but setting it too high may reduce the model’s ability to distinguish
foreground from background.

## Multi-GPU training

MIST uses PyTorch's DistributedDataParallel (DDP) for multi-GPU data
parallelism. By default, MIST will use all visible GPUs on a given system.
However, you may specify which GPUs MIST uses for training with the `--gpus`
flag for the `mist_run_all` and `mist_train` commands.

For example, if your system has eight GPUs available, but you only want to
use GPUs `0` and `5`, then run `mist_run_all <other arguments> --gpus 0 5`.

Note that `training.hardware.master_port` controls the port used for multi-GPU
communication. If you run multiple jobs on the same machine, ensure each job
uses a different `master_port` value.
